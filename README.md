<h1 align='center'>JoyTTS: LLM-based Spoken Chatbot With Voice Cloning</h1>

<div align='center'>
    <a href='https://github.com/zhoufangru' target='_blank'>Zhou Fangru</a>â€ƒ
    <a href='https://github.com/zhaojun060708' target='_blank'>Jun Zhao</a>â€ƒ
    Guoxin Wang
</div>
<div align='center'>
    JD Health International Inc.
</div>

<br>
<div align='center'>
    <a href='https://huggingface.co/jdh-algo/JoyTTS-v1'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Model-yellow'></a>
</div>
<br>

## ğŸ“– Introduction

JoyTTS is an end-to-end spoken chatbot that combines large language models (LLM) with text-to-speech (TTS) technology, featuring voice cloning capabilities. This project is built upon the open-source MiniCPM-o and CosyVoice2 models and trained on 2000 hours of conversational data. We have also provided the complete training code to facilitate further development and optimization by the community. On the testing machine seed-tts-zh, it achieves a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.


### ğŸ§³ Framework

![Network](assets/æµå¼ç»“æ„å›¾.jpg "Network")

### ğŸ¬ Demo


<table>
<tr>
<td width="25%">
é—®é¢˜
</td>
<td width="25%">
å­™æ‚Ÿç©º
</td>
<td width="25%">
çŒªå…«æˆ’
</td>
<td width="25%">
æ—é»›ç‰
</td>
</tr>
<tr>
<td width="25%">
å‚è€ƒéŸ³é¢‘
</td>
<td width="25%">
    
https://github.com/user-attachments/assets/c7b1c703-3513-44a6-91a6-24729ae2c652
</td>
<td width="25%">

https://github.com/user-attachments/assets/284acc8c-f15f-41ef-9af2-05d6aba70692
</td>
<td width="25%">

https://github.com/user-attachments/assets/46a9e2b4-aabf-46ea-99b2-b41bccd1b683
</td>
</tr>
<tr>
<td width="25%">
â€œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·â€
</td>
<td width="25%">

https://github.com/user-attachments/assets/56ac6536-72b9-4e93-819d-136f091c0b04
</td>
<td width="25%">

https://github.com/user-attachments/assets/7d4057f5-d139-4055-bc04-f081632b452b
</td>
<td width="25%">

https://github.com/user-attachments/assets/b3a1fc21-73f3-444a-8fe9-67f98f4b1a2c
</td>
</tr>
<tr>
<td width="25%">
â€œä¸­å›½çš„å…¨ç§°æ˜¯ä»€ä¹ˆâ€
</td>
<td width="25%">
    
https://github.com/user-attachments/assets/0e8bce5b-0a70-4e5b-9010-46baa0c0cdd3
</td>
<td width="25%">

https://github.com/user-attachments/assets/051c3c14-f261-4c97-b281-95c80eb409c5
</td>
<td width="25%">

https://github.com/user-attachments/assets/f58ba707-86cd-48cf-916e-dde6217e83a4
</td>
</tr>
<tr>
<td width="25%">
â€œæ‰‹æœºéƒ½æœ‰å“ªäº›ä½œç”¨ï¼Ÿâ€
</td>
<td width="25%">

https://github.com/user-attachments/assets/79418095-470e-4b16-830c-9ff835df6852
</td>
<td width="25%">

https://github.com/user-attachments/assets/38d5db81-2077-47d5-8cf2-45c8911349ae
</td>
<td width="25%">

https://github.com/user-attachments/assets/bdaa784e-2419-4620-a94d-be87cbb79c7f
</td>
</tr>
</table>


### ğŸ’» Results on SEED test-zh

|             Model             | SS $\uparrow$ | WER $\downarrow$ |
| :---------------------------: | :------: | :----: |
|     gpt-sovits     |  0.55  | 5.13 |
|     cosyvoice2     |   **0.748**   |  **1.45**  |
|     Minicpm-o      |  0.57  |  -   |
|     JoyTTS         |  0.73  | 5.09 |


## âš™ï¸ Installation

### 1. Create Conda env

``` sh
conda create -n JoyTTS -y python=3.10
conda activate JoyTTS
conda install -y -c conda-forge pynini==2.1.5
pip install -r requirements_JoyTTS.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
cp third_party/deepspeed/elasticity/elastic_agent.py $conda_envs/JoyTTS/lib/python3.10/site-packages/deepspeed/elasticity/elastic_agent.py
```

### 2. Model download

``` sh
huggingface-cli download --resume-download --repo-type model jdh-algo/JoyTTS-v1 --local-dir pretrained_models
```

## ğŸš€ Inference

### 1. Use spk_id

1. Change the prompt info in pretrained_models/prompt_info.py
2. Delete pretrained_models/spk2info.pt
3. Run the following command

```shell
python inference.py --question 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ' --spk_id 'å­™æ‚Ÿç©º' 
```

### 2. Use prompt info
```shell
python inference.py --question 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ' --prompt_text 'prompt text' --prompt_wav the_path_of_prompt_wav 
```
The result will be saved in "output.wav"


### 3. Gradio demo
```shell
python gradio_demo.py
```


## ğŸš§ Fine-tuning
### 1. Data preparation

#### 1. Prepare conversation data for LLM-Chat
<details>
  <summary>
    <b>example (normalized_data.json) with 1 sample.</b>
  </summary>

```
  [
    {
      "utt": "0",
      "reference": 'æ•°å­—é“¶è¡Œ ã€Šæ•°å­—é“¶è¡Œã€‹æ˜¯2015å¹´12æœˆæ–°ä¸–ç•Œå‡ºç‰ˆç¤¾å‡ºç‰ˆçš„é‡‘èç±»å›¾ä¹¦ï¼Œè¯¥ä¹¦çš„ä½œè€…æ˜¯åˆ˜æŒ¯å‹ã€‚ ä¹¦ å æ•°å­—é“¶è¡Œ ä½œ è€… åˆ˜æŒ¯å‹ ç±» åˆ« é‡‘è å‡ºç‰ˆç¤¾ æ–°ä¸–ç•Œå‡ºç‰ˆç¤¾ å‡ºç‰ˆæ—¶é—´ 2015å¹´12æœˆ é¡µ æ•° 290 é¡µ å®š ä»· 48.8 å¼€ æœ¬ 16 å¼€ è£… å¸§ å¹³è£… ISBN 978-7-5104-5480-6 æ•°å­—é“¶è¡Œå†…å®¹ç®€ä»‹ ã€Šæ•°å­—é“¶è¡Œã€‹è®¤ä¸ºï¼Œä¼ ç»Ÿé“¶è¡Œè‹¥è¦æ›´å¥½åœ°å­˜ç»­ï¼Œå¿…é¡»å®ç°è‡ªæˆ‘è¿›åŒ–ã€‚æœ¬ä¹¦è¿˜å°†å¸¦ä½ ç©¿è¶Šå†å²ï¼Œè§£è¯»ç°ä»£é‡‘èè¡Œä¸šçš„å‘å±•è½¨é“ï¼ŒçŸ¥å¤é‰´ä»Šï¼ŒæŠŠæ¡æœªæ¥é“¶è¡Œçš„å‘å±•æ€ç»´è„‰ç»œã€‚ æ•°å­—é“¶è¡Œä½œè€…ç®€ä»‹ åˆ˜æŒ¯å‹æ˜¯å…¨çƒç¬¬ä¸€æ‰¹è‡´åŠ›äºåŠ å¯†æ•°å­—èµ„äº§ã€æ•°å­—è´§å¸ã€è™šæ‹Ÿè´§å¸ã€å•†ä¸šç§¯åˆ†ã€å…±èµ¢ç»æµæ¨¡å¼ã€å…¨æ°‘å…è´¹ç»æµé¢†åŸŸç ”ç©¶ã€åº”ç”¨ã€æ¨å¹¿çš„èµ„æ·±ä¸“å®¶ï¼Œæ–°é”ä½œå®¶ã€‚ä¸»è¦è‘—ä½œæœ‰:ã€Šé¢ è¦†ã€‹ã€Š3åˆ†é’Ÿè®©åˆ«äººè®°ä½ä½ ã€‹ã€Šäº’è”ç½‘+ã€‹ã€Šæ•°å­—é“¶è¡Œã€‹ã€Šè®©ä¸–ç•Œå…è´¹ã€‹ã€‚ æ•°å­—é“¶è¡Œä½œå“ç›®å½•',
      "conversations": [
            {
              'role': 'user', 
              'content': 'è¯·é—®è¿™æœ¬ã€Šæ•°å­—é“¶è¡Œã€‹æ˜¯ä»€ä¹ˆæ—¶å€™å‡ºç‰ˆçš„ï¼Ÿ'
            }, 
            {
                'role': 'assistant', 
                'content': 'è¿™æœ¬ã€Šæ•°å­—é“¶è¡Œã€‹æ˜¯2015å¹´12æœˆå‡ºç‰ˆçš„ã€‚å®ƒæ˜¯ä¸€æœ¬é‡‘èç±»å›¾ä¹¦ï¼Œç”±æ–°ä¸–ç•Œå‡ºç‰ˆç¤¾å‡ºç‰ˆã€‚äº†è§£è¿™æœ¬ä¹¦çš„å‡ºç‰ˆæ—¶é—´å¯ä»¥å¸®åŠ©ä½ äº†è§£å®ƒçš„æ—¶æ•ˆæ€§ï¼Œä»è€Œäº†è§£è¯¥ä¹¦æ‰€æè¿°çš„å†…å®¹å’Œè§‚ç‚¹æ˜¯å¦ä»ç„¶é€‚ç”¨äºå½“å‰çš„é‡‘èç¯å¢ƒã€‚'
            },   
            {
                'role': 'user', 
                'content': 'ä½œè€…åˆ˜æŒ¯å‹éƒ½å†™è¿‡å“ªäº›å…¶ä»–ä¹¦ç±ï¼Ÿ'
            },
            {
                'role': 'assistant', 
                'content': 'åˆ˜æŒ¯å‹æ˜¯ä¸€ä½æ–°é”ä½œå®¶ï¼Œä»–çš„ä¸»è¦è‘—ä½œæœ‰ã€Šé¢ è¦†ã€‹ã€ã€Š3åˆ†é’Ÿè®©åˆ«äººè®°ä½ä½ ã€‹ã€ã€Šäº’è”ç½‘+ã€‹ã€ã€Šæ•°å­—é“¶è¡Œã€‹å’Œã€Šè®©ä¸–ç•Œå…è´¹ã€‹ã€‚ä»–æ˜¯å…¨çƒç¬¬ä¸€æ‰¹è‡´åŠ›äºåŠ å¯†æ•°å­—èµ„äº§ã€æ•°å­—è´§å¸ã€è™šæ‹Ÿè´§å¸ã€å•†ä¸šç§¯åˆ†ã€å…±èµ¢ç»æµæ¨¡å¼ã€å…¨æ°‘å…è´¹ç»æµé¢†åŸŸç ”ç©¶ã€åº”ç”¨ã€æ¨å¹¿çš„èµ„æ·±ä¸“å®¶ã€‚äº†è§£ä»–çš„å…¶ä»–è‘—ä½œå¯ä»¥å¸®åŠ©ä½ æ›´å…¨é¢åœ°äº†è§£ä»–çš„æ€æƒ³å’Œç ”ç©¶é¢†åŸŸã€‚'
            }, 
        ]
    },
  ]
```
</details>

#### 2. Prepare data for  LLM-TTS
To enhance the efficiency of training the LLM-TTS model, we pre-generated hidden states using the already trained LLM-Chat model.

##### launch server
```shell
cd prepare_data_sever and python sever_llmanswer.py
cd prepare_data_sever and python sever_tts.py
```
##### generate parquet file
```shell
cd prepare_data_generate
python 0_generate_datalist.py --data_root your_data_root --data_name your_data_name
python 1_generate_llmanswer.py --data_root your_data_root --data_name your_data_name
python 2_generate_tts.py --data_root your_data_root --data_name your_data_name
python 3_split_train_eval_offline.py --data_root your_data_root --data_name your_data_name
python 4_make_parquet_list.py --data_root your_data_root --data_name your_data_name
```
The generated data will be saved in {your_data_root}/parquet/{your_data_name}

#### 3. Prepare data for end-to-end training
##### launch server
```shell
cd prepare_data_sever and python sever_tts.py
```
##### generate parquet file
```shell
cd prepare_data_generate_end2end
python 0_generate_datalist.py --data_root your_data_root --data_name your_data_name
python 2_generate_tts.py --data_root your_data_root --data_name your_data_name
python 3_split_train_eval_offline.py --data_root your_data_root --data_name your_data_name
python 4_make_parquet_list.py --data_root your_data_root --data_name your_data_name
```
The generated data will be saved in {your_data_root}/parquet_uselabel/{your_data_name}


### 2. Start train
#### 1. LLM-TTS model only

``` shell
cd examples/end2end
ln -s your_data_root data
sh run.sh #set stage=1
```
#### 2. end-to-end training

``` shell
cd examples/end2end
ln -s your_data_root data
sh run.sh #set stage=2
```

## ğŸ’ TODO
- [x] training and inference codes.
- [x] trained models and technical report.
- [x] freestyle dialogue model with voice cloning.
- [ ] vllm support.
- [ ] a better and faster model.
- [ ] automatic emotion control.


## ğŸ“ Citations

If you find our work helpful, please consider citing us:

```
@misc{zhou2025joytts,
  title={JoyTTS: LLM-based Spoken Chatbot With Voice Cloning}, 
  author={Fangru Zhou and Jun Zhao and Guoxin Wang},
  year={2025},
  howpublished = {\url{https://jdh-algo.github.io/JoyTTS}},
}
```

## ğŸ¤ Acknowledgments

We would like to thank the contributors to the [MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file), [CosyVoice2](https://github.com/FunAudioLLM/CosyVoice2https://github.com/FunAudioLLM/CosyVoice2)repositories, for their open research and extraordinary work.
